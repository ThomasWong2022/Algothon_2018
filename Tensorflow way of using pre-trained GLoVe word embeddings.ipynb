{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'glove.6B.50d.txt'\n",
    "# The vocab is a list of words or marks.\n",
    "# The embedding is the huge 2-d array with all the word vectors.\n",
    "# We initialize the embedding size to be the number of column of the embedding array.\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadGloVe(filename)\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = len(embd[0])\n",
    "embedding = np.asarray(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),\n",
    "                trainable=False, name=\"W\")\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "embedding_init = W.assign(embedding_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you have raw documents, the first thing you need to do is to build a vocabulary, which will map each word into an id.\n",
    "# tf.nn.embedding_lookup(W, input_x)\n",
    "# where W is the huge embedding matrix, input_x is a tensor with ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First init the vocab processor by passing in a max_document_length,\n",
    "# in default, shorter sentences would be padded by zeros.\n",
    "# Then we fit the processor by the vocab list to build the word-id maps.\n",
    "# Finally, use the processor to transform from real raw documents.\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "#init vocab processor\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "#fit the vocab from glove\n",
    "pretrain = vocab_processor.fit(vocab)\n",
    "#transform inputs\n",
    "x = np.array(list(vocab_processor.transform(your_raw_input)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
